{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAlbfpGF5aUh"
      },
      "source": [
        "# Hands-on with Spark (Structured Streaming)\n",
        "\n",
        "![spark](https://cdn-images-1.medium.com/max/300/1*c8CtvqKJDVUnMoPGujF5fA.png)\n",
        "\n",
        "In the previous lesson, we learnt about Spark SQL, Dataframes and Pandas API. In this lesson, we will continue with the Structured Streaming.\n",
        "\n",
        "Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. You can express your streaming computation the same way you would express a batch computation on static data. The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.\n",
        "\n",
        "You can use the Dataset/DataFrame API to express streaming aggregations, event-time windows, stream-to-batch joins, etc. The computation is executed on the same optimized Spark SQL engine. Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs. In short, Structured Streaming provides **fast, scalable, fault-tolerant, end-to-end exactly-once** stream processing without the user having to reason about streaming.\n",
        "\n",
        "Internally, by default, Structured Streaming queries are processed using a micro-batch processing engine, which processes data streams as a series of small batch jobs thereby achieving end-to-end latencies as low as 100 milliseconds and exactly-once fault-tolerance guarantees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0M79JSa6Oqv"
      },
      "source": [
        "## Installing and Initializing Spark\n",
        "\n",
        "First, like previously, we'll need to install Spark and its dependencies:\n",
        "\n",
        "1.   Java 8\n",
        "2.   Apache Spark with Hadoop\n",
        "3.   Findspark (used to locate the Spark in the system)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KE17krsq6Rd6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
        "\n",
        "# set the options to connect to our Kafka cluster\n",
        "options = {\n",
        "    # \"kafka.sasl.jaas.config\": 'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"YnJhdmUtZmlzaC0xMTQ2MyQSvwXBuLOQsV1W7YffuC8cDaZcA3fKQwakMhnQGgg\" password=\"MDUxNjc4YzEtYzYxNy00NTE1LWEwNWYtMDBhODRlZmE0OGJm\";',\n",
        "    # \"kafka.sasl.mechanism\": \"SCRAM-SHA-256\",\n",
        "    # \"kafka.security.protocol\" : \"SASL_SSL\",\n",
        "    \"kafka.bootstrap.servers\": 'localhost:9092',\n",
        "    \"subscribe\": 'pizza-orders',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ra8Ncwms6UsD"
      },
      "outputs": [],
      "source": [
        "# import findspark\n",
        "# findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lhi1xuPB6VfD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/09/05 20:36:22 WARN Utils: Your hostname, 02PBMTT.local resolves to a loopback address: 127.0.0.1; using 10.204.15.81 instead (on interface en0)\n",
            "25/09/05 20:36:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            ":: loading settings :: url = jar:file:/opt/miniconda3/envs/kk/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /Users/aiml/.ivy2/cache\n",
            "The jars for the packages stored in: /Users/aiml/.ivy2/jars\n",
            "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8d686a78-27a8-40d3-8c6c-1e42680e0fe9;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
            "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
            "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
            "\tfound org.lz4#lz4-java;1.8.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
            "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
            "\tfound commons-logging#commons-logging;1.1.3 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
            ":: resolution report :: resolve 658ms :: artifacts dl 21ms\n",
            "\t:: modules in use:\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
            "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
            "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
            "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
            "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
            "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
            "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-8d686a78-27a8-40d3-8c6c-1e42680e0fe9\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 11 already retrieved (0kB/14ms)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/09/05 20:36:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/09/05 20:36:26 WARN TransportChannelHandler: Exception in connection from /0.0.0.0:0\n",
            "java.io.IOException: Socket is not connected\n",
            "\tat java.base/sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
            "\tat java.base/sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
            "\tat java.base/sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276)\n",
            "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:233)\n",
            "\tat java.base/sun.nio.ch.IOUtil.read(IOUtil.java:223)\n",
            "\tat java.base/sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:356)\n",
            "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:258)\n",
            "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
            "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n",
            "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
            "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:722)\n",
            "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)\n",
            "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)\n",
            "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)\n",
            "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
            "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "25/09/05 20:36:26 ERROR TransportClient: Failed to send RPC /jars/org.apache.commons_commons-pool2-2.11.1.jar to <unknown remote>: io.netty.channel.StacklessClosedChannelException\n",
            "io.netty.channel.StacklessClosedChannelException\n",
            "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
            "25/09/05 20:36:26 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from <unknown remote> is closed\n",
            "25/09/05 20:36:26 ERROR SparkContext: Error initializing SparkContext.\n",
            "java.io.IOException: Failed to send RPC /jars/org.apache.commons_commons-pool2-2.11.1.jar to <unknown remote>: io.netty.channel.StacklessClosedChannelException\n",
            "\tat org.apache.spark.network.client.TransportClient$2.handleFailure(TransportClient.java:164)\n",
            "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369)\n",
            "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n",
            "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n",
            "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n",
            "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n",
            "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n",
            "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
            "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
            "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
            "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)\n",
            "\tat io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:110)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)\n",
            "\tat io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:302)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
            "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
            "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
            "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n",
            "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
            "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n",
            "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
            "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: io.netty.channel.StacklessClosedChannelException\n",
            "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
            "25/09/05 20:36:26 ERROR Utils: Uncaught exception in thread Thread-3\n",
            "java.lang.NullPointerException\n",
            "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.org$apache$spark$scheduler$local$LocalSchedulerBackend$$stop(LocalSchedulerBackend.scala:173)\n",
            "\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.stop(LocalSchedulerBackend.scala:144)\n",
            "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:931)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2785)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n",
            "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
            "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:685)\n",
            "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
            "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "25/09/05 20:36:26 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.IOException: Failed to send RPC /jars/org.apache.commons_commons-pool2-2.11.1.jar to <unknown remote>: io.netty.channel.StacklessClosedChannelException\n\tat org.apache.spark.network.client.TransportClient$2.handleFailure(TransportClient.java:164)\n\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)\n\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)\n\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)\n\tat io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:110)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)\n\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)\n\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)\n\tat io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:302)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: io.netty.channel.StacklessClosedChannelException\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLearnSparkStreaming\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/kk/lib/python3.10/site-packages/pyspark/sql/session.py:269\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
            "File \u001b[0;32m/opt/miniconda3/envs/kk/lib/python3.10/site-packages/pyspark/context.py:483\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
            "File \u001b[0;32m/opt/miniconda3/envs/kk/lib/python3.10/site-packages/pyspark/context.py:197\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mudf_profiler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
            "File \u001b[0;32m/opt/miniconda3/envs/kk/lib/python3.10/site-packages/pyspark/context.py:282\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
            "File \u001b[0;32m/opt/miniconda3/envs/kk/lib/python3.10/site-packages/pyspark/context.py:402\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/miniconda3/envs/kk/lib/python3.10/site-packages/py4j/java_gateway.py:1585\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1579\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1581\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1584\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1585\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1589\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
            "File \u001b[0;32m/opt/miniconda3/envs/kk/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.IOException: Failed to send RPC /jars/org.apache.commons_commons-pool2-2.11.1.jar to <unknown remote>: io.netty.channel.StacklessClosedChannelException\n\tat org.apache.spark.network.client.TransportClient$2.handleFailure(TransportClient.java:164)\n\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)\n\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)\n\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)\n\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)\n\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)\n\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)\n\tat io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:110)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)\n\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)\n\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)\n\tat io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:302)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: io.netty.channel.StacklessClosedChannelException\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"LearnSparkStreaming\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "VYTXdMSv6cpC",
        "outputId": "e0817f69-3087-4e2c-e643-5ba3ed477a11"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/09/05 20:36:45 ERROR Utils: Uncaught exception in thread executor-heartbeater\n",
            "java.lang.ExceptionInInitializerError\n",
            "\tat org.apache.spark.metrics.ProcessTreeMetrics$.getMetricValues(ExecutorMetricType.scala:93)\n",
            "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1(ExecutorMetrics.scala:103)\n",
            "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1$adapted(ExecutorMetrics.scala:102)\n",
            "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
            "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
            "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
            "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
            "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
            "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
            "\tat org.apache.spark.executor.ExecutorMetrics$.getCurrentMetrics(ExecutorMetrics.scala:102)\n",
            "\tat org.apache.spark.executor.ExecutorMetricsPoller.poll(ExecutorMetricsPoller.scala:82)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1031)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:238)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
            "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
            "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
            "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "Caused by: java.lang.NullPointerException\n",
            "\tat org.apache.spark.executor.ProcfsMetricsGetter.isProcfsAvailable$lzycompute(ProcfsMetricsGetter.scala:62)\n",
            "\tat org.apache.spark.executor.ProcfsMetricsGetter.isProcfsAvailable(ProcfsMetricsGetter.scala:51)\n",
            "\tat org.apache.spark.executor.ProcfsMetricsGetter.<init>(ProcfsMetricsGetter.scala:48)\n",
            "\tat org.apache.spark.executor.ProcfsMetricsGetter$.<init>(ProcfsMetricsGetter.scala:232)\n",
            "\tat org.apache.spark.executor.ProcfsMetricsGetter$.<clinit>(ProcfsMetricsGetter.scala)\n",
            "\t... 22 more\n"
          ]
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36cRSgfTNXXA"
      },
      "source": [
        "## Read and Analyze Kafka stream in \"Batch\" Mode\n",
        "\n",
        "Let's start with reading and analyzing our `pizza-orders` kafka topic in the usual \"batch\" mode of Spark SQL and Dataframes. This is akin to the batch queries we did in the previous lesson. In this case we are taking the messages with the earliest to latest offsets of the topic as a single \"batch\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6fZDMWC26dJ0"
      },
      "outputs": [],
      "source": [
        "pizza_df = spark.read.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJLqrFeXOXpG",
        "outputId": "56b04137-b3cc-4231-e23a-ea7f16977956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "|                 key|               value|       topic|partition|offset|           timestamp|timestampType|\n",
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     0|2025-03-18 21:52:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     1|2025-03-18 21:52:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     2|2025-03-18 21:52:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     3|2025-03-18 21:53:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     4|2025-03-18 21:53:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     5|2025-03-18 21:53:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     6|2025-03-18 21:53:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     7|2025-03-18 21:53:...|            0|\n",
            "|[7B 22 73 68 6F 7...|[7B 22 69 64 22 3...|pizza-orders|        0|     8|2025-03-18 21:53:...|            0|\n",
            "+--------------------+--------------------+------------+---------+------+--------------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5--uyDySFSEB",
        "outputId": "30d90a0a-8d05-4ae7-8ad6-dcdba0677464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                 key|               value|\n",
            "+--------------------+--------------------+\n",
            "|{\"shop\": \"Circula...|{\"id\": 1, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 2, \"shop\":...|\n",
            "|{\"shop\": \"Luigis ...|{\"id\": 3, \"shop\":...|\n",
            "|{\"shop\": \"Circula...|{\"id\": 4, \"shop\":...|\n",
            "|{\"shop\": \"Its-a m...|{\"id\": 5, \"shop\":...|\n",
            "|{\"shop\": \"Circula...|{\"id\": 6, \"shop\":...|\n",
            "|{\"shop\": \"Marios ...|{\"id\": 7, \"shop\":...|\n",
            "|{\"shop\": \"Ill Mak...|{\"id\": 8, \"shop\":...|\n",
            "|{\"shop\": \"Mammami...|{\"id\": 9, \"shop\":...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m_zjPQ6_FUVE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StringType, IntegerType, LongType, DoubleType, StructType, ArrayType, StructField"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "owyJGHliQdBG"
      },
      "outputs": [],
      "source": [
        "pizza_schema = StructType([\n",
        "  StructField(\"pizzaName\", StringType()),\n",
        "  StructField(\"additionalToppings\", ArrayType(StringType())),\n",
        "])\n",
        "\n",
        "order_schema = StructType([\n",
        "  StructField(\"address\", StringType()),\n",
        "  StructField(\"id\", IntegerType()),\n",
        "  StructField(\"name\", StringType()),\n",
        "  StructField(\"phoneNumber\", StringType()),\n",
        "  StructField(\"shop\", StringType()),\n",
        "  StructField(\"cost\", DoubleType()),\n",
        "  StructField(\"pizzas\", ArrayType(pizza_schema)),\n",
        "  StructField(\"timestamp\", LongType()),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8_KAqsS7Q4_5"
      },
      "outputs": [],
      "source": [
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7MWOtWPRM-0",
        "outputId": "af678ae7-e0be-4117-88b9-27cdd9fd8e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: struct (nullable = true)\n",
            " |    |-- address: string (nullable = true)\n",
            " |    |-- id: integer (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- phoneNumber: string (nullable = true)\n",
            " |    |-- shop: string (nullable = true)\n",
            " |    |-- cost: double (nullable = true)\n",
            " |    |-- pizzas: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- pizzaName: string (nullable = true)\n",
            " |    |    |    |-- additionalToppings: array (nullable = true)\n",
            " |    |    |    |    |-- element: string (containsNull = true)\n",
            " |    |-- timestamp: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mio4VUyERh1H",
        "outputId": "36c7f7d8-3f03-41dd-9425-5859d45bd940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|timestamp              |value                                                                                                                                                                                                                                                                                                                        |\n",
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|2025-03-18 21:52:55.545|{21087 Calvin Plains\\nJonesland, NY 76392, 1, Jessica Smith, 001-701-915-3000, Circular Pi Pizzeria, 3.3, [{Marinara, []}], 1742305975545}                                                                                                                                                                                   |\n",
            "|2025-03-18 21:52:57.549|{19721 Drew Key\\nNew Donaldport, NH 05690, 2, Roger Brown, 475-943-3780x8105, Luigis Pizza, 45.29, [{Margherita, [üêü tuna, üçå banana]}, {Diavola, []}, {Salami, [üçÖ tomato, üçå banana]}, {Diavola, [üßÖ onion, üçå banana]}], 1742305977548}                                                                                   |\n",
            "|2025-03-18 21:52:59.553|{278 Phillips Crossing Apt. 661\\nPort Maryfurt, GU 86535, 3, Matthew Williams, 924-927-0965x759, Luigis Pizza, 24.15, [{Salami, [üßÖ onion]}, {Diavola, []}], 1742305979553}                                                                                                                                                  |\n",
            "|2025-03-18 21:53:00.559|{547 Brenda Walks\\nLake Ronaldborough, CT 52490, 4, Corey Rodriguez, 001-629-242-5711x630, Circular Pi Pizzeria, 12.82, [{Salami, [ü´ë green peppers]}, {Diavola, [üßÖ onion]}, {Salami, [üå∂Ô∏è hot pepper]}, {Diavola, [üçå banana, üßÄ blue cheese]}, {Margherita, [ü•ö egg, ü•ö egg]}], 1742305980559}                            |\n",
            "|2025-03-18 21:53:02.57 |{752 Brandon Expressway Apt. 815\\nNorth Elizabeth, AR 84161, 5, Stephen Stevens, 253.786.2934, Its-a me! Mario Pizza!, 4.53, [{Salami, [üå∂Ô∏è hot pepper, üçå banana]}, {Margherita, [üêü tuna, üçç pineapple]}, {Marinara, [üêü tuna, ü´ë green peppers, üçì strawberry]}], 1742305982568}                                          |\n",
            "|2025-03-18 21:53:04.58 |{144 Ross Port\\nAliciafurt, KY 28535, 6, Ricky Sexton, 8667536484, Circular Pi Pizzeria, 24.47, [{Margherita, [ü•ì bacon, üçÖ tomato]}, {Marinara, [üêü tuna]}, {Mari & Monti, [üßÑ garlic]}, {Salami, []}], 1742305984579}                                                                                                      |\n",
            "|2025-03-18 21:53:05.586|{49595 Gary Port Suite 707\\nNorth Michaelfort, CA 18853, 7, Russell Sanders, (873)972-6401x764, Marios Pizza, 27.58, [{Diavola, [ü´í olives, üå∂Ô∏è hot pepper]}, {Salami, [üçÖ tomato]}], 1742305985585}                                                                                                                         |\n",
            "|2025-03-18 21:53:07.594|{4189 Madison Glens Suite 206\\nLake Vickiborough, TN 20792, 8, Stephanie Rivera, (544)795-9493x841, Ill Make You a Pizza You Cant Refuse, 39.86, [{Diavola, []}, {Diavola, [ü´ë green peppers, üå∂Ô∏è hot pepper]}, {Diavola, [üçç pineapple, üçç pineapple]}, {Diavola, [üçå banana, üßÑ garlic, ü´ë green peppers]}], 1742305987593}|\n",
            "|2025-03-18 21:53:09.599|{77289 Dustin Villages\\nLake Frank, NY 99386, 9, Kelli Hamilton DDS, 244-779-3581, Mammamia Pizza, 38.72, [{Mari & Monti, []}], 1742305989598}                                                                                                                                                                               |\n",
            "+-----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdYUKmrygE7_"
      },
      "source": [
        "We can use _dot notation_ to select the field within a `Struct`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt_TUX3_edrK",
        "outputId": "8907d09f-371c-4c04-8912-a91348b40c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+\n",
            "| cost|\n",
            "+-----+\n",
            "|  3.3|\n",
            "|45.29|\n",
            "|24.15|\n",
            "|12.82|\n",
            "| 4.53|\n",
            "|24.47|\n",
            "|27.58|\n",
            "|39.86|\n",
            "|38.72|\n",
            "+-----+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.select(\"value.cost\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgfNZXpVgXz_"
      },
      "source": [
        "Computing the \"total revenue\" per shop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqZHVVFggUYv",
        "outputId": "5c140144-c405-44d5-ff32-e38bdd0cab0b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 3:>                                                          (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------+-----------------------+\n",
            "|shop                                |sum(value.cost AS cost)|\n",
            "+------------------------------------+-----------------------+\n",
            "|Ill Make You a Pizza You Cant Refuse|39.86                  |\n",
            "|Luigis Pizza                        |69.44                  |\n",
            "|Mammamia Pizza                      |38.72                  |\n",
            "|Its-a me! Mario Pizza!              |4.53                   |\n",
            "|Marios Pizza                        |27.58                  |\n",
            "|Circular Pi Pizzeria                |40.59                  |\n",
            "+------------------------------------+-----------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "parsed_df.groupBy(\"value.shop\").sum(\"value.cost\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GuvYqRChU_v"
      },
      "source": [
        "> 1. Count the no. of orders by shop.\n",
        "> 2. Compute the avg revenue by shop and sort by highest to lowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fEuCpW_Pgk1Q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import min, max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8IhcetFiY24",
        "outputId": "7af16123-be01-4746-e028-2d0f66212964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+-----------------------+\n",
            "|min(timestamp)         |max(timestamp)         |\n",
            "+-----------------------+-----------------------+\n",
            "|2025-03-18 21:52:55.545|2025-03-18 21:53:09.599|\n",
            "+-----------------------+-----------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.select(min(\"timestamp\"), max(\"timestamp\")).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MDIcbPs74B3"
      },
      "source": [
        "## Read and Analyze Kafka stream in \"Streaming\" Mode\n",
        "\n",
        "The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model. You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an incremental query on the *unbounded input table*. Let‚Äôs understand this model in more detail.\n",
        "\n",
        "Consider the input data stream as the ‚ÄúInput Table‚Äù. Every data item that is arriving on the stream is like a new row being appended to the Input Table.\n",
        "\n",
        "![concept](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)\n",
        "\n",
        "A query on the input will generate the ‚ÄúResult Table‚Äù. Every trigger interval (say, every 1 second), new rows get appended to the Input Table, which eventually updates the Result Table. Whenever the result table gets updated, we would want to write the changed result rows to an external sink.\n",
        "\n",
        "![result table](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)\n",
        "\n",
        "To illustrate the use of this model, let‚Äôs understand the model in context of a word count model. The first lines DataFrame is the input table, and the final wordCounts DataFrame is the result table. Note that the query on streaming lines DataFrame to generate wordCounts is exactly the same as it would be a static DataFrame. However, when this query is started, Spark will continuously check for new data from the socket connection. If there is new data, Spark will run an ‚Äúincremental‚Äù query that combines the previous running counts with the new data to compute updated counts, as shown below.\n",
        "\n",
        "![example](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9A-q7BY9F_F"
      },
      "source": [
        "Event-time is the time embedded in the data itself. For many applications, you may want to operate on this event-time.\n",
        "\n",
        "For example, if you want to get the number of events generated by IoT devices every minute, then you probably want to use the time when the data was generated (that is, event-time in the data), rather than the time Spark receives them.\n",
        "\n",
        "This event-time is very naturally expressed in this model ‚Äì each event from the devices is a row in the table, and event-time is a column value in the row. This allows window-based aggregations (e.g. number of events every minute) to be just a special type of grouping and aggregation on the event-time column ‚Äì each time window is a group and each row can belong to multiple windows/groups. Therefore, such event-time-window-based aggregation queries can be defined consistently on both a static dataset (e.g. from collected device events logs) as well as on a data stream, making the life of the user much easier.\n",
        "\n",
        "Furthermore, this model naturally handles data that has arrived later than expected based on its event-time. Since Spark is updating the Result Table, it has full control over updating old aggregates when there is late data, as well as cleaning up old aggregates to limit the size of intermediate state data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TKfF1Mc9STFX"
      },
      "outputs": [],
      "source": [
        "pizza_df = spark.readStream.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss8wgrNDkwuW",
        "outputId": "d0f3d31d-d229-46f7-db9b-9a893ca8f44b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pizza_df.isStreaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grlYCt8Ujg3G",
        "outputId": "3048feba-75ec-40bd-9f1f-6b9bd8788622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- key: binary (nullable = true)\n",
            " |-- value: binary (nullable = true)\n",
            " |-- topic: string (nullable = true)\n",
            " |-- partition: integer (nullable = true)\n",
            " |-- offset: long (nullable = true)\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- timestampType: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pizza_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "G_vL4jkTjl2v"
      },
      "outputs": [],
      "source": [
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox00HZHql7_N",
        "outputId": "b02edc33-ebfd-4867-9141-9036946e2320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/03/18 22:17:30 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/z9/_l65135549l_v6k4d3tz369r0000gn/T/temporary-8074f8b4-f14f-499b-9e91-cd2ca4efdbe1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "25/03/18 22:17:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<pyspark.sql.streaming.StreamingQuery at 0x10f759900>"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---------+-----+\n",
            "|timestamp|value|\n",
            "+---------+-----+\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.writeStream.format(\"console\").start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ETDCxskFZ9",
        "outputId": "9b7b3a27-13c7-447a-d5cd-2a02d0d55892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: struct (nullable = true)\n",
            " |    |-- address: string (nullable = true)\n",
            " |    |-- id: integer (nullable = true)\n",
            " |    |-- name: string (nullable = true)\n",
            " |    |-- phoneNumber: string (nullable = true)\n",
            " |    |-- shop: string (nullable = true)\n",
            " |    |-- cost: double (nullable = true)\n",
            " |    |-- pizzas: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- pizzaName: string (nullable = true)\n",
            " |    |    |    |-- additionalToppings: array (nullable = true)\n",
            " |    |    |    |    |-- element: string (containsNull = true)\n",
            " |    |-- timestamp: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parsed_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0nnh-Nrjmxd",
        "outputId": "fba91660-248f-4c34-a615-c86c56fe7edf"
      },
      "outputs": [],
      "source": [
        "query = parsed_df.where(\"value.cost > 10\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "zY8eDSs9kLan",
        "outputId": "c4ae7110-9517-4f0d-bf8a-d767fb4c8c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/03/18 22:08:40 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/z9/_l65135549l_v6k4d3tz369r0000gn/T/temporary-5fa09916-1c31-4cf2-bb84-be9dae426409. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "25/03/18 22:08:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---------+-----+\n",
            "|timestamp|value|\n",
            "+---------+-----+\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query = parsed_df \\\n",
        "    .writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "# query.awaitTermination() # stops the script from exiting\n",
        "# query.isActive\n",
        "# query.recentProgress\n",
        "# query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:08:40.658Z',\n",
              "  'batchId': 0,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'addBatch': 23,\n",
              "   'getBatch': 1,\n",
              "   'latestOffset': 3254,\n",
              "   'queryPlanning': 7,\n",
              "   'triggerExecution': 3482,\n",
              "   'walCommit': 90},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': None,\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:08:54.147Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 4, 'triggerExecution': 4},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:09:04.156Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 2, 'triggerExecution': 2},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:09:14.169Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 2, 'triggerExecution': 2},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:09:24.179Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 4},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:09:34.193Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 2, 'triggerExecution': 2},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:09:44.203Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 2, 'triggerExecution': 2},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:09:54.217Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 5, 'triggerExecution': 5},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:10:04.234Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 12, 'triggerExecution': 12},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:10:14.221Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 55, 'triggerExecution': 55},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:10:24.280Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 5, 'triggerExecution': 5},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:10:34.291Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 12, 'triggerExecution': 12},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:10:44.309Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:10:54.314Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 2, 'triggerExecution': 2},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:11:04.326Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:11:14.332Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 6, 'triggerExecution': 6},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:11:24.337Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 10, 'triggerExecution': 10},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:11:34.351Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 8, 'triggerExecution': 8},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:11:44.364Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 15, 'triggerExecution': 15},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:11:54.386Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 6, 'triggerExecution': 6},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:12:04.401Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 6, 'triggerExecution': 6},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:12:14.414Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 3, 'triggerExecution': 3},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:12:24.424Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 4, 'triggerExecution': 4},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:12:34.438Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 10, 'triggerExecution': 10},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:12:44.460Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 13, 'triggerExecution': 13},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:12:54.474Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 7, 'triggerExecution': 7},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:04.477Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 12, 'triggerExecution': 12},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:14.492Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 16, 'triggerExecution': 16},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:24.497Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 12, 'triggerExecution': 12},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:34.508Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 8, 'triggerExecution': 8},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 9}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 9}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:36.636Z',\n",
              "  'batchId': 1,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 52.631578947368425,\n",
              "  'processedRowsPerSecond': 0.7757951900698216,\n",
              "  'durationMs': {'addBatch': 699,\n",
              "   'getBatch': 1,\n",
              "   'latestOffset': 2,\n",
              "   'queryPlanning': 18,\n",
              "   'triggerExecution': 1289,\n",
              "   'walCommit': 242},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 9}},\n",
              "    'endOffset': {'pizza-orders': {'0': 10}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 10}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 52.631578947368425,\n",
              "    'processedRowsPerSecond': 0.7757951900698216,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:37.925Z',\n",
              "  'batchId': 2,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 0.7757951900698216,\n",
              "  'processedRowsPerSecond': 0.8176614881439084,\n",
              "  'durationMs': {'addBatch': 609,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 6,\n",
              "   'queryPlanning': 6,\n",
              "   'triggerExecution': 1223,\n",
              "   'walCommit': 224},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 10}},\n",
              "    'endOffset': {'pizza-orders': {'0': 11}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 11}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 0.7757951900698216,\n",
              "    'processedRowsPerSecond': 0.8176614881439084,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:39.635Z',\n",
              "  'batchId': 3,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 83.33333333333333,\n",
              "  'processedRowsPerSecond': 0.9433962264150942,\n",
              "  'durationMs': {'addBatch': 581,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 2,\n",
              "   'queryPlanning': 7,\n",
              "   'triggerExecution': 1060,\n",
              "   'walCommit': 250},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 11}},\n",
              "    'endOffset': {'pizza-orders': {'0': 12}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 12}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 83.33333333333333,\n",
              "    'processedRowsPerSecond': 0.9433962264150942,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:41.642Z',\n",
              "  'batchId': 4,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 62.5,\n",
              "  'processedRowsPerSecond': 0.8857395925597874,\n",
              "  'durationMs': {'addBatch': 583,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 5,\n",
              "   'queryPlanning': 6,\n",
              "   'triggerExecution': 1129,\n",
              "   'walCommit': 267},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 12}},\n",
              "    'endOffset': {'pizza-orders': {'0': 13}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 13}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 62.5,\n",
              "    'processedRowsPerSecond': 0.8857395925597874,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:42.772Z',\n",
              "  'batchId': 5,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 0.8849557522123894,\n",
              "  'processedRowsPerSecond': 0.9337068160597572,\n",
              "  'durationMs': {'addBatch': 590,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 3,\n",
              "   'queryPlanning': 5,\n",
              "   'triggerExecution': 1071,\n",
              "   'walCommit': 234},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 13}},\n",
              "    'endOffset': {'pizza-orders': {'0': 14}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 14}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 0.8849557522123894,\n",
              "    'processedRowsPerSecond': 0.9337068160597572,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:44.655Z',\n",
              "  'batchId': 6,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 66.66666666666667,\n",
              "  'processedRowsPerSecond': 0.9578544061302682,\n",
              "  'durationMs': {'addBatch': 582,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 3,\n",
              "   'queryPlanning': 5,\n",
              "   'triggerExecution': 1044,\n",
              "   'walCommit': 225},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 14}},\n",
              "    'endOffset': {'pizza-orders': {'0': 15}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 15}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 66.66666666666667,\n",
              "    'processedRowsPerSecond': 0.9578544061302682,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:45.700Z',\n",
              "  'batchId': 7,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 0.9569377990430623,\n",
              "  'processedRowsPerSecond': 0.9416195856873822,\n",
              "  'durationMs': {'addBatch': 558,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 5,\n",
              "   'queryPlanning': 4,\n",
              "   'triggerExecution': 1062,\n",
              "   'walCommit': 224},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 15}},\n",
              "    'endOffset': {'pizza-orders': {'0': 16}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 16}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 0.9569377990430623,\n",
              "    'processedRowsPerSecond': 0.9416195856873822,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:46.763Z',\n",
              "  'batchId': 8,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 0.9407337723424272,\n",
              "  'processedRowsPerSecond': 0.9267840593141798,\n",
              "  'durationMs': {'addBatch': 591,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 2,\n",
              "   'queryPlanning': 18,\n",
              "   'triggerExecution': 1079,\n",
              "   'walCommit': 229},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 16}},\n",
              "    'endOffset': {'pizza-orders': {'0': 17}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 17}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 0.9407337723424272,\n",
              "    'processedRowsPerSecond': 0.9267840593141798,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:48.673Z',\n",
              "  'batchId': 9,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 55.55555555555556,\n",
              "  'processedRowsPerSecond': 0.9469696969696969,\n",
              "  'durationMs': {'addBatch': 549,\n",
              "   'getBatch': 5,\n",
              "   'latestOffset': 5,\n",
              "   'queryPlanning': 6,\n",
              "   'triggerExecution': 1056,\n",
              "   'walCommit': 241},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 17}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 55.55555555555556,\n",
              "    'processedRowsPerSecond': 0.9469696969696969,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:13:59.731Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 5, 'triggerExecution': 5},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:14:09.737Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 5, 'triggerExecution': 5},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:14:19.741Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 4, 'triggerExecution': 4},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:14:29.749Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 7, 'triggerExecution': 7},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:14:39.754Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 5, 'triggerExecution': 5},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:14:49.760Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 7, 'triggerExecution': 7},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:14:59.770Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 8, 'triggerExecution': 8},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:09.777Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 4, 'triggerExecution': 4},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:19.789Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 0,\n",
              "  'inputRowsPerSecond': 0.0,\n",
              "  'processedRowsPerSecond': 0.0,\n",
              "  'durationMs': {'latestOffset': 2, 'triggerExecution': 2},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 18}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 18}},\n",
              "    'numInputRows': 0,\n",
              "    'inputRowsPerSecond': 0.0,\n",
              "    'processedRowsPerSecond': 0.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 0}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:25.471Z',\n",
              "  'batchId': 10,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 52.631578947368425,\n",
              "  'processedRowsPerSecond': 0.89126559714795,\n",
              "  'durationMs': {'addBatch': 587,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 2,\n",
              "   'queryPlanning': 4,\n",
              "   'triggerExecution': 1122,\n",
              "   'walCommit': 268},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 18}},\n",
              "    'endOffset': {'pizza-orders': {'0': 19}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 19}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 52.631578947368425,\n",
              "    'processedRowsPerSecond': 0.89126559714795,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:26.593Z',\n",
              "  'batchId': 11,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 0.89126559714795,\n",
              "  'processedRowsPerSecond': 1.0141987829614605,\n",
              "  'durationMs': {'addBatch': 553,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 12,\n",
              "   'queryPlanning': 5,\n",
              "   'triggerExecution': 986,\n",
              "   'walCommit': 241},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 19}},\n",
              "    'endOffset': {'pizza-orders': {'0': 20}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 20}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 0.89126559714795,\n",
              "    'processedRowsPerSecond': 1.0141987829614605,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:28.481Z',\n",
              "  'batchId': 12,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 50.0,\n",
              "  'processedRowsPerSecond': 1.0471204188481675,\n",
              "  'durationMs': {'addBatch': 548,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 8,\n",
              "   'queryPlanning': 4,\n",
              "   'triggerExecution': 955,\n",
              "   'walCommit': 194},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 20}},\n",
              "    'endOffset': {'pizza-orders': {'0': 21}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 21}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 50.0,\n",
              "    'processedRowsPerSecond': 1.0471204188481675,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:29.472Z',\n",
              "  'batchId': 13,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 55.55555555555556,\n",
              "  'processedRowsPerSecond': 0.9442870632672333,\n",
              "  'durationMs': {'addBatch': 553,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 6,\n",
              "   'queryPlanning': 6,\n",
              "   'triggerExecution': 1059,\n",
              "   'walCommit': 306},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 21}},\n",
              "    'endOffset': {'pizza-orders': {'0': 22}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 22}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 55.55555555555556,\n",
              "    'processedRowsPerSecond': 0.9442870632672333,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:31.475Z',\n",
              "  'batchId': 14,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 66.66666666666667,\n",
              "  'processedRowsPerSecond': 0.998003992015968,\n",
              "  'durationMs': {'addBatch': 556,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 9,\n",
              "   'queryPlanning': 4,\n",
              "   'triggerExecution': 1002,\n",
              "   'walCommit': 218},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 22}},\n",
              "    'endOffset': {'pizza-orders': {'0': 23}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 23}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 66.66666666666667,\n",
              "    'processedRowsPerSecond': 0.998003992015968,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:32.477Z',\n",
              "  'batchId': 15,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 0.998003992015968,\n",
              "  'processedRowsPerSecond': 1.050420168067227,\n",
              "  'durationMs': {'addBatch': 547,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 11,\n",
              "   'queryPlanning': 3,\n",
              "   'triggerExecution': 952,\n",
              "   'walCommit': 214},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 23}},\n",
              "    'endOffset': {'pizza-orders': {'0': 24}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 24}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 0.998003992015968,\n",
              "    'processedRowsPerSecond': 1.050420168067227,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:33.476Z',\n",
              "  'batchId': 16,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 62.5,\n",
              "  'processedRowsPerSecond': 1.050420168067227,\n",
              "  'durationMs': {'addBatch': 553,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 4,\n",
              "   'queryPlanning': 5,\n",
              "   'triggerExecution': 952,\n",
              "   'walCommit': 211},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 24}},\n",
              "    'endOffset': {'pizza-orders': {'0': 25}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 25}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 62.5,\n",
              "    'processedRowsPerSecond': 1.050420168067227,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:34.490Z',\n",
              "  'batchId': 17,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 62.5,\n",
              "  'processedRowsPerSecond': 1.0,\n",
              "  'durationMs': {'addBatch': 557,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 4,\n",
              "   'queryPlanning': 3,\n",
              "   'triggerExecution': 1000,\n",
              "   'walCommit': 230},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 25}},\n",
              "    'endOffset': {'pizza-orders': {'0': 26}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 26}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 62.5,\n",
              "    'processedRowsPerSecond': 1.0,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}},\n",
              " {'id': '590d73ca-0eaa-4c3b-a9d7-47794ea5a5cd',\n",
              "  'runId': '88fc1108-a1e5-43f6-8593-a3f277ef2afe',\n",
              "  'name': None,\n",
              "  'timestamp': '2025-03-18T14:15:35.490Z',\n",
              "  'batchId': 18,\n",
              "  'numInputRows': 1,\n",
              "  'inputRowsPerSecond': 1.0,\n",
              "  'processedRowsPerSecond': 1.0559662090813096,\n",
              "  'durationMs': {'addBatch': 544,\n",
              "   'getBatch': 0,\n",
              "   'latestOffset': 5,\n",
              "   'queryPlanning': 4,\n",
              "   'triggerExecution': 947,\n",
              "   'walCommit': 208},\n",
              "  'stateOperators': [],\n",
              "  'sources': [{'description': 'KafkaV2[Subscribe[pizza-orders]]',\n",
              "    'startOffset': {'pizza-orders': {'0': 26}},\n",
              "    'endOffset': {'pizza-orders': {'0': 27}},\n",
              "    'latestOffset': {'pizza-orders': {'0': 27}},\n",
              "    'numInputRows': 1,\n",
              "    'inputRowsPerSecond': 1.0,\n",
              "    'processedRowsPerSecond': 1.0559662090813096,\n",
              "    'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
              "     'maxOffsetsBehindLatest': '0',\n",
              "     'minOffsetsBehindLatest': '0'}}],\n",
              "  'sink': {'description': 'org.apache.spark.sql.execution.streaming.ConsoleTable$@6f418af6',\n",
              "   'numOutputRows': 1}}]"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 19\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{4298 Schroeder C...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 19\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 19\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 19\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 19\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{4298 Schroeder C...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 19\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{4298 Schroeder C...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{4298 Schroeder C...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{4298 Schroeder C...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{4298 Schroeder C...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 20\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0346 Joyce Pine\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 20\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0346 Joyce Pine\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 20\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 20\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0346 Joyce Pine\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0346 Joyce Pine\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 20\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 20\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0346 Joyce Pine\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0346 Joyce Pine\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 21\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0603 Brandi Path...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 21\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 21\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0603 Brandi Path...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0603 Brandi Path...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 21\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0603 Brandi Path...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 21\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0603 Brandi Path...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 21\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{0603 Brandi Path...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 22\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 22\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 22\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{94556 Chaney Cam...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{94556 Chaney Cam...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{94556 Chaney Cam...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 22\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 22\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{94556 Chaney Cam...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 22\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{94556 Chaney Cam...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{94556 Chaney Cam...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 23\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{3903 Jose Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 23\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 23\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{3903 Jose Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{3903 Jose Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 23\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 23\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 23\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{3903 Jose Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{3903 Jose Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{3903 Jose Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 24\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 24\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 24\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 24\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 24\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 24\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{06867 Sean Sprin...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{06867 Sean Sprin...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{06867 Sean Sprin...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{06867 Sean Sprin...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{06867 Sean Sprin...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{06867 Sean Sprin...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 25\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 25\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 25\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{5781 John Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{5781 John Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{5781 John Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 25\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{5781 John Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 25\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 25\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{5781 John Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:15:...|{5781 John Fords\\...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 26\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 26\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 26\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 26\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{42486 Kelly Traf...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{42486 Kelly Traf...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{42486 Kelly Traf...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{42486 Kelly Traf...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 26\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 26\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{42486 Kelly Traf...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{42486 Kelly Traf...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 27\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{37881 Clayton Co...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 27\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{37881 Clayton Co...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 27\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 27\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "Batch: 27\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{37881 Clayton Co...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{37881 Clayton Co...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{37881 Clayton Co...|\n",
            "+--------------------+--------------------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 27\n",
            "-------------------------------------------\n",
            "+--------------------+--------------------+\n",
            "|           timestamp|               value|\n",
            "+--------------------+--------------------+\n",
            "|2025-03-18 22:16:...|{37881 Clayton Co...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "query.recentProgress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In the next few cells, we will provide a complete example of counting the number of pizza shops with orders, as the orders stream in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First check if any streams are active\n",
        "spark.streams.active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If there are any active streams, stop them\n",
        "for q in spark.streams.active:\n",
        "    print(f\"Stopping query: {q.name}\")\n",
        "    q.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete example with counting\n",
        "pizza_df = spark.readStream.format('kafka')\\\n",
        "    .options(**options)\\\n",
        "    .load()\n",
        "\n",
        "parsed_df = pizza_df.select(\"timestamp\", from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"value\")) #.groupBy(\"value.shop\").count()\n",
        "\n",
        "shop_counts = parsed_df.groupBy(\"value.shop\").count()\n",
        "\n",
        "# outputMode(\"complete\") rewrites all aggregated results every trigger ‚Äî good for full snapshots.\n",
        "# outputMode(\"update\") - only updated rows are written\n",
        "query = shop_counts \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"console\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You have come to the end of this exercise.\n",
        "\n",
        "To delete the Kafka topic `pizza-orders`, use the command below in your terminal:\n",
        "\n",
        "`./kafka-topics.sh --delete --topic pizza-orders --bootstrap-server localhost:9092`\n",
        "\n",
        "To exit Kafka in your terminal, type `exit`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kk",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
